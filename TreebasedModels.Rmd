---
title: "Trees-Based Models"
author: "Sandesh Bhandari"
date: "5/2/2020"
output: html_document
---

## Here we evaluate trees-based models

First, we import the libraries and the data sets.

```{r warning = F, message = F}
# Clean up R environment
rm(list = ls())

# Load in packages
library(tidyverse) # Data wrangling, ggplot, etc.
library(knitr)     # This is to make pretty tables (see kable() )
library(tree)
library(randomForest)
library(gbm)
library(pROC)

# Read in training and testing data
model_test <- 
  readRDS("./Data/model_test.RDS") 
model_train <- 
  readRDS("./Data/model_train.RDS")

# Global options
set.seed(14)
theme_set(theme_bw()) # Set a better ggplot theme
options(digits=3)     # Set digits to 3 to avoid too many values
```



# Seasonal Vaccine Prediction

```{r}
seasonal_train <- 
  model_train %>%
  select(-h1n1_vaccine, -respondent_id) %>%
  mutate(seasonal_vaccine= as_factor(seasonal_vaccine))

seasonal_test <-
  model_test %>%
  select(-h1n1_vaccine, -respondent_id)%>%
  mutate(seasonal_vaccine= as_factor(seasonal_vaccine))

seasonal_train <- seasonal_train %>%
  mutate_if(sapply(seasonal_train, is.character), as.factor)

seasonal_test <- seasonal_test %>%
  mutate_if(sapply(seasonal_test, is.character), as.factor)
```

### Tree

```{r, warning=F}
tree_seasonal <- tree(seasonal_vaccine~.,seasonal_train)

plot(tree_seasonal)

text(tree_seasonal,pretty=0)
```

Above, we can see what the tree looks like. Our tree only uses 3 variables for this classification: opinion_seas_risk, doctor_recc_seasonal and opinion_seas_vacc_effective.

We can see below how accurately this tree performs on the training set.

```{r, warning=F}
tree.pred=predict(tree_seasonal,seasonal_train,type="class")

# Confusion matrix for the tree
table(tree.pred, seasonal_train$seasonal_vaccine)

# Proportion of correct predictions in training set
mean(tree.pred== seasonal_train$seasonal_vaccine)
```

It looks like an accuracy of 73.4% is better than chance. However, an error rate of 26.6% is still not great. We can look into ways to prune this tree to see if they can lead to improved performance.

### Pruned Tree

We can use cross-validation to determine what sized tree might be best for our purposes. We will use misclassification rate tog guide the cross-validation.

```{r, warning=F}
cv_tree_seasonal <-cv.tree(tree_seasonal,FUN=prune.misclass)

names(cv_tree_seasonal)
cv_tree_seasonal

par(mfrow=c(1,2))

plot(cv_tree_seasonal$size,cv_tree_seasonal$dev,type="b")

```

From the output above, it looks like cross-validation error is lowest when the tree size is 5 or 6. A tree of size 6 is the original tree we started with. So, it doesn't seem that using size of 5 or 6 will give much different results but let's try using a tree of size 5.

```{r, warning=FALSE}
prune_tree_seasonal=prune.misclass(tree_seasonal,best=5)

prunetree.pred=predict(prune_tree_seasonal,seasonal_train,type="class")

# Confusion matrix for the tree
table(prunetree.pred, seasonal_train$seasonal_vaccine)

# Proportion of correct predictions in training set
mean(prunetree.pred== seasonal_train$seasonal_vaccine)
```

Once again, we get an accuracy of 73.4%. This suggests that pruning the above tree doesn't really affect the performance and they lead to the exact same predictions. Therefore, let's just look at the test set performance of the full tree.

### Tree model performance on test set

```{r, warning=FALSE}
tree.pred.test=predict(tree_seasonal,seasonal_test,type="class")

# Confusion matrix for the tree
table(tree.pred.test, seasonal_test$seasonal_vaccine)

# Proportion of correct predictions in training set
mean(tree.pred.test== seasonal_test$seasonal_vaccine)

```

The tree has test set accuracy of 73.2% (test error rate of 26.8%).

Based on these analysis, it doesn't look like using a tree would be a great option. However, we can still use methods like bagging and random forest to combine different trees and evaluate whether these combinations are better fit for this data.

### Bagging

```{r, warning=FALSE}
set.seed(56)

bag_seasonal <- randomForest(seasonal_vaccine~.,seasonal_train, mtry=26,importance=TRUE)
bag_seasonal
```

We can see that bagging results in a out of bag (OOB) error rate of just 22.6% (accuracy of 77.4%), which is much better than 26.6% training error rate we got for the pruned and unpruned trees. This suggests that this model might perform better on test set.

```{r, warning=FALSE}
varImpPlot(bag_seasonal)
```

### Bagged model performance on test set

```{r, warning=FALSE}
bag.pred.test=predict(bag_seasonal,seasonal_test,type="class")

# Confusion matrix for the bagged model
table(bag.pred.test, seasonal_test$seasonal_vaccine)

# Proportion of correct predictions in training set
mean(bag.pred.test== seasonal_test$seasonal_vaccine)

```

The bagging model has a test set accuracy of 77% (test error rate of 23%).

### Random Forest

We used mtry=26 so that all available features were considered for each split of the tree. Instead of using all 26 features at each of the step, we could also only use a select few chosen randomly each time. This means that we will just be conducting a random forest analysis. We are going to use the default mtry value, which is square root of 26 (around 5). Let's see if that improves our performance.

```{r, warning=FALSE}
set.seed(56)

rf_seasonal <- randomForest(seasonal_vaccine~.,seasonal_train, importance=TRUE)
rf_seasonal
```

Here, we can see that using random forest with 500 trees and 5 random features considered at each split results in an out of bag (OOB) error rate of 21.6% (an accuracy of 78.4%) which is better than the error rate obtained using the bagging method.

```{r, warning=FALSE}
varImpPlot(rf_seasonal)
```

Now let's evaluate this model's performance on test set.

### Random Forest model performance on test set

```{r, warning=FALSE}
rf.pred.test=predict(rf_seasonal,seasonal_test,type="class")

# Confusion matrix for the random forest
table(rf.pred.test, seasonal_test$seasonal_vaccine)

# Proportion of correct predictions in training set
mean(rf.pred.test== seasonal_test$seasonal_vaccine)

```

The random forest model had a test set accuracy of 77.8% (a test error rate of 22.2%).

### Boosting

We can also use boosting and evaluate its performance. A 5-fold cross-validation was also conducted to determine the optimal number of trees.

```{r, warning=FALSE}
set.seed(56)


boost_seasonal <- gbm(seasonal_vaccine~.,(seasonal_train %>% mutate(seasonal_vaccine=as.integer(levels(seasonal_vaccine)[seasonal_vaccine]))), distribution = "bernoulli",n.trees=5000,interaction.depth=1,shrinkage=0.01,cv.folds=5)
boost_seasonal
summary(boost_seasonal)
```

Our cross-validation suggested that the most optimal number of trees to use for this data would be 4997. 

Let's evaluate this models performance on the test set.

### Boosted model performance on test set

```{r, warning=FALSE}
boost.pred.test=predict(boost_seasonal,seasonal_test,type="response",n.trees=4997)

boost.pred.test = round(boost.pred.test)

# Confusion matrix for the boosted model
table(boost.pred.test, (seasonal_test %>% mutate(seasonal_vaccine=as.integer(levels(seasonal_vaccine)[seasonal_vaccine])))$seasonal_vaccine)

# Proportion of correct predictions in training set
mean(boost.pred.test== (seasonal_test %>% mutate(seasonal_vaccine=as.integer(levels(seasonal_vaccine)[seasonal_vaccine])))$seasonal_vaccine)

```

Here, we see that our boosted model with 4997 trees resulted in a test set accuracy of 77.8% (test error rate of 22.2%).




# H1N1 Vaccine Prediction

```{r}
h1n1_train <- 
  model_train %>%
  select(-seasonal_vaccine, -respondent_id) %>%
  mutate(h1n1_vaccine= as_factor(h1n1_vaccine))

h1n1_test <-
  model_test %>%
  select(-seasonal_vaccine, -respondent_id)%>%
  mutate(h1n1_vaccine= as_factor(h1n1_vaccine))

h1n1_train <- h1n1_train %>%
  mutate_if(sapply(h1n1_train, is.character), as.factor)

h1n1_test <- h1n1_test %>%
  mutate_if(sapply(h1n1_test, is.character), as.factor)
```

### Tree

```{r, warning=F}
tree_h1n1 <- tree(h1n1_vaccine~.,h1n1_train)

plot(tree_h1n1)

text(tree_h1n1,pretty=0)
```

Above, we can see what the tree looks like. Our tree only uses 4 variables for this classification: opinion_h1n1_risk, doctor_recc_h1n1,  opinion_h1n1_vacc_effective, and health_worker.

We can see below how accurately this tree performs on the training set.

```{r, warning=F}
tree.pred=predict(tree_h1n1,h1n1_train,type="class")

# Confusion matrix for the tree
table(tree.pred, h1n1_train$h1n1_vaccine)

# Proportion of correct predictions in training set
mean(tree.pred== h1n1_train$h1n1_vaccine)
```

It looks like we get an accuracy of 81.5% (an error rate of 18.5%), which is pretty good. Let's see if pruning this tree can lead to improved performance.

### Pruned Tree

We can use cross-validation to determine the pruned tree size. We will use misclassification rate to guide the cross-validation.

```{r, warning=F}
cv_tree_h1n1 <-cv.tree(tree_h1n1,FUN=prune.misclass)

names(cv_tree_h1n1)
cv_tree_h1n1

par(mfrow=c(1,2))

plot(cv_tree_h1n1$size,cv_tree_h1n1$dev,type="b")

```

From the output above, it looks like cross-validation error is lowest when the tree size is 5 or 6. A tree of size 6 is the original tree we started with. So, it doesn't seem that using size of 5 or 6 will give much different results but let's try using a tree of size 5.

```{r, warning=FALSE}
prune_tree_h1n1=prune.misclass(tree_h1n1,best=5)

prunetree.pred=predict(prune_tree_h1n1,h1n1_train,type="class")

# Confusion matrix for the tree
table(prunetree.pred, h1n1_train$h1n1_vaccine)

# Proportion of correct predictions in training set
mean(prunetree.pred== h1n1_train$h1n1_vaccine)
```

Once again, we get an accuracy of 81.5%. This suggests that pruning the above tree doesn't really affect the performance and they lead to the exact same predictions. Therefore, let's just look at the test set performance of the full tree.

### Tree model performance on test set

```{r, warning=FALSE}
tree.pred.test=predict(tree_h1n1,h1n1_test,type="class")

# Confusion matrix for the tree
table(tree.pred.test, h1n1_test$h1n1_vaccine)

# Proportion of correct predictions in training set
mean(tree.pred.test== h1n1_test$h1n1_vaccine)

```

The tree has test set accuracy of 81.4% (test error rate of 18.6%).

Next we use methods like bagging and random forest to combine different trees and evaluate whether these combinations are better fit for this data.

### Bagging

```{r, warning=FALSE}
set.seed(56)

bag_h1n1 <- randomForest(h1n1_vaccine~.,h1n1_train, mtry=26,importance=TRUE)
bag_h1n1
```

We can see that bagging results in a out of bag (OOB) error rate of just 17.6% (accuracy of 82.4%), which is much better than 18.5% training error rate we got for the pruned and unpruned trees. This suggests that this model might perform better on test set.

### Bagged model performance on test set

```{r, warning=FALSE}
bag.pred.test=predict(bag_h1n1,h1n1_test,type="class")

# Confusion matrix for the bagged model
table(bag.pred.test, h1n1_test$h1n1_vaccine)

# Proportion of correct predictions in training set
mean(bag.pred.test== h1n1_test$h1n1_vaccine)

```

The bagging model has a test set accuracy of 82.5% (test error rate of 17.5%).

```{r, warning=FALSE}
varImpPlot(bag_h1n1)
```

### Random Forest

Let's see if using random forest improves our performance.

```{r, warning=FALSE}
set.seed(56)

rf_h1n1 <- randomForest(h1n1_vaccine~.,h1n1_train, importance=TRUE)
rf_h1n1
```

Here, we can see that using random forest with 500 trees and 5 random features considered at each split results in an out of bag (OOB) error rate of 17.1% (an accuracy of 82.9%) which is slightly better than the error rate obtained using the bagging method.

```{r, warning=FALSE}
varImpPlot(rf_h1n1)
```

Now let's evaluate this model's performance on test set.

### Random Forest model performance on test set

```{r, warning=FALSE}
rf.pred.test=predict(rf_h1n1,h1n1_test,type="class")

# Confusion matrix for the random forest
table(rf.pred.test, h1n1_test$h1n1_vaccine)

# Proportion of correct predictions in training set
mean(rf.pred.test== h1n1_test$h1n1_vaccine)

```

The random forest model had a test set accuracy of 83.1% (a test error rate of 16.9%).

### Boosting

We can also use boosting and evaluate its performance. A 5-fold cross-validation was also conducted to determine the optimal number of trees.

```{r, warning=FALSE}
set.seed(56)


boost_h1n1 <- gbm(h1n1_vaccine~.,(h1n1_train %>% mutate(h1n1_vaccine=as.integer(levels(h1n1_vaccine)[h1n1_vaccine]))), distribution = "bernoulli",n.trees=5000,interaction.depth=1,shrinkage=0.01,cv.folds=5)
boost_h1n1
summary(boost_h1n1)
```

Our cross-validation suggested that the most optimal number of trees to use for this data would be 3911. 

Let's evaluate this models performance on the test set.

### Boosted model performance on test set

```{r, warning=FALSE}
boost.pred.test=predict(boost_h1n1,h1n1_test,type="response",n.trees=3911)

boost.pred.test = round(boost.pred.test)

# Confusion matrix for the boosted model
table(boost.pred.test, (h1n1_test %>% mutate(h1n1_vaccine=as.integer(levels(h1n1_vaccine)[h1n1_vaccine])))$h1n1_vaccine)

# Proportion of correct predictions in training set
mean(boost.pred.test== (h1n1_test %>% mutate(h1n1_vaccine=as.integer(levels(h1n1_vaccine)[h1n1_vaccine])))$h1n1_vaccine)

```

Here, we see that our boosted model with 3911 trees resulted in a test set accuracy of 83% (test error rate of 17%).






